{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    " \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    " \n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"data_imu_birlestirilmis.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>ax</th>\n",
       "      <th>ay</th>\n",
       "      <th>az</th>\n",
       "      <th>gx</th>\n",
       "      <th>gy</th>\n",
       "      <th>gz</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>Class Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1.091892</td>\n",
       "      <td>8.707838</td>\n",
       "      <td>1.765225</td>\n",
       "      <td>-0.036396</td>\n",
       "      <td>0.063694</td>\n",
       "      <td>-0.036396</td>\n",
       "      <td>1900-01-01 17:44:42.000000</td>\n",
       "      <td>not_falling</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1.083568</td>\n",
       "      <td>8.732087</td>\n",
       "      <td>1.765318</td>\n",
       "      <td>-0.036781</td>\n",
       "      <td>0.064346</td>\n",
       "      <td>-0.036686</td>\n",
       "      <td>1900-01-01 17:44:42.020000</td>\n",
       "      <td>not_falling</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1.075247</td>\n",
       "      <td>8.756325</td>\n",
       "      <td>1.765411</td>\n",
       "      <td>-0.037166</td>\n",
       "      <td>0.064996</td>\n",
       "      <td>-0.036976</td>\n",
       "      <td>1900-01-01 17:44:42.040000</td>\n",
       "      <td>not_falling</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1.066932</td>\n",
       "      <td>8.780539</td>\n",
       "      <td>1.765503</td>\n",
       "      <td>-0.037551</td>\n",
       "      <td>0.065642</td>\n",
       "      <td>-0.037265</td>\n",
       "      <td>1900-01-01 17:44:42.060000</td>\n",
       "      <td>not_falling</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1.058625</td>\n",
       "      <td>8.804720</td>\n",
       "      <td>1.765596</td>\n",
       "      <td>-0.037936</td>\n",
       "      <td>0.066281</td>\n",
       "      <td>-0.037553</td>\n",
       "      <td>1900-01-01 17:44:42.080000</td>\n",
       "      <td>not_falling</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51645</th>\n",
       "      <td>51645</td>\n",
       "      <td>1.611913</td>\n",
       "      <td>8.047413</td>\n",
       "      <td>-3.202833</td>\n",
       "      <td>-0.106623</td>\n",
       "      <td>0.084735</td>\n",
       "      <td>-0.050371</td>\n",
       "      <td>1900-01-01 17:59:15.899794</td>\n",
       "      <td>not_falling</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51646</th>\n",
       "      <td>51646</td>\n",
       "      <td>1.626502</td>\n",
       "      <td>8.043946</td>\n",
       "      <td>-3.201882</td>\n",
       "      <td>-0.118920</td>\n",
       "      <td>0.123886</td>\n",
       "      <td>-0.071806</td>\n",
       "      <td>1900-01-01 17:59:15.919794</td>\n",
       "      <td>not_falling</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51647</th>\n",
       "      <td>51647</td>\n",
       "      <td>1.641042</td>\n",
       "      <td>8.040466</td>\n",
       "      <td>-3.200937</td>\n",
       "      <td>-0.131261</td>\n",
       "      <td>0.163164</td>\n",
       "      <td>-0.093307</td>\n",
       "      <td>1900-01-01 17:59:15.939794</td>\n",
       "      <td>not_falling</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51648</th>\n",
       "      <td>51648</td>\n",
       "      <td>1.655544</td>\n",
       "      <td>8.036974</td>\n",
       "      <td>-3.199998</td>\n",
       "      <td>-0.143635</td>\n",
       "      <td>0.202538</td>\n",
       "      <td>-0.114858</td>\n",
       "      <td>1900-01-01 17:59:15.959793</td>\n",
       "      <td>not_falling</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51649</th>\n",
       "      <td>51649</td>\n",
       "      <td>1.670022</td>\n",
       "      <td>8.033476</td>\n",
       "      <td>-3.199062</td>\n",
       "      <td>-0.156031</td>\n",
       "      <td>0.241975</td>\n",
       "      <td>-0.136441</td>\n",
       "      <td>1900-01-01 17:59:15.979793</td>\n",
       "      <td>not_falling</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>51650 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0        ax        ay        az        gx        gy        gz  \\\n",
       "0               0  1.091892  8.707838  1.765225 -0.036396  0.063694 -0.036396   \n",
       "1               1  1.083568  8.732087  1.765318 -0.036781  0.064346 -0.036686   \n",
       "2               2  1.075247  8.756325  1.765411 -0.037166  0.064996 -0.036976   \n",
       "3               3  1.066932  8.780539  1.765503 -0.037551  0.065642 -0.037265   \n",
       "4               4  1.058625  8.804720  1.765596 -0.037936  0.066281 -0.037553   \n",
       "...           ...       ...       ...       ...       ...       ...       ...   \n",
       "51645       51645  1.611913  8.047413 -3.202833 -0.106623  0.084735 -0.050371   \n",
       "51646       51646  1.626502  8.043946 -3.201882 -0.118920  0.123886 -0.071806   \n",
       "51647       51647  1.641042  8.040466 -3.200937 -0.131261  0.163164 -0.093307   \n",
       "51648       51648  1.655544  8.036974 -3.199998 -0.143635  0.202538 -0.114858   \n",
       "51649       51649  1.670022  8.033476 -3.199062 -0.156031  0.241975 -0.136441   \n",
       "\n",
       "                        timestamp   Class Name  \n",
       "0      1900-01-01 17:44:42.000000  not_falling  \n",
       "1      1900-01-01 17:44:42.020000  not_falling  \n",
       "2      1900-01-01 17:44:42.040000  not_falling  \n",
       "3      1900-01-01 17:44:42.060000  not_falling  \n",
       "4      1900-01-01 17:44:42.080000  not_falling  \n",
       "...                           ...          ...  \n",
       "51645  1900-01-01 17:59:15.899794  not_falling  \n",
       "51646  1900-01-01 17:59:15.919794  not_falling  \n",
       "51647  1900-01-01 17:59:15.939794  not_falling  \n",
       "51648  1900-01-01 17:59:15.959793  not_falling  \n",
       "51649  1900-01-01 17:59:15.979793  not_falling  \n",
       "\n",
       "[51650 rows x 9 columns]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder,StandardScaler\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder=LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Class Name\"]=label_encoder.fit_transform(df[\"Class Name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['falling' 'not_falling']\n"
     ]
    }
   ],
   "source": [
    "print(label_encoder.classes_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>ax</th>\n",
       "      <th>ay</th>\n",
       "      <th>az</th>\n",
       "      <th>gx</th>\n",
       "      <th>gy</th>\n",
       "      <th>gz</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>Class Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1.091892</td>\n",
       "      <td>8.707838</td>\n",
       "      <td>1.765225</td>\n",
       "      <td>-0.036396</td>\n",
       "      <td>0.063694</td>\n",
       "      <td>-0.036396</td>\n",
       "      <td>1900-01-01 17:44:42.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1.083568</td>\n",
       "      <td>8.732087</td>\n",
       "      <td>1.765318</td>\n",
       "      <td>-0.036781</td>\n",
       "      <td>0.064346</td>\n",
       "      <td>-0.036686</td>\n",
       "      <td>1900-01-01 17:44:42.020000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1.075247</td>\n",
       "      <td>8.756325</td>\n",
       "      <td>1.765411</td>\n",
       "      <td>-0.037166</td>\n",
       "      <td>0.064996</td>\n",
       "      <td>-0.036976</td>\n",
       "      <td>1900-01-01 17:44:42.040000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1.066932</td>\n",
       "      <td>8.780539</td>\n",
       "      <td>1.765503</td>\n",
       "      <td>-0.037551</td>\n",
       "      <td>0.065642</td>\n",
       "      <td>-0.037265</td>\n",
       "      <td>1900-01-01 17:44:42.060000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1.058625</td>\n",
       "      <td>8.804720</td>\n",
       "      <td>1.765596</td>\n",
       "      <td>-0.037936</td>\n",
       "      <td>0.066281</td>\n",
       "      <td>-0.037553</td>\n",
       "      <td>1900-01-01 17:44:42.080000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51645</th>\n",
       "      <td>51645</td>\n",
       "      <td>1.611913</td>\n",
       "      <td>8.047413</td>\n",
       "      <td>-3.202833</td>\n",
       "      <td>-0.106623</td>\n",
       "      <td>0.084735</td>\n",
       "      <td>-0.050371</td>\n",
       "      <td>1900-01-01 17:59:15.899794</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51646</th>\n",
       "      <td>51646</td>\n",
       "      <td>1.626502</td>\n",
       "      <td>8.043946</td>\n",
       "      <td>-3.201882</td>\n",
       "      <td>-0.118920</td>\n",
       "      <td>0.123886</td>\n",
       "      <td>-0.071806</td>\n",
       "      <td>1900-01-01 17:59:15.919794</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51647</th>\n",
       "      <td>51647</td>\n",
       "      <td>1.641042</td>\n",
       "      <td>8.040466</td>\n",
       "      <td>-3.200937</td>\n",
       "      <td>-0.131261</td>\n",
       "      <td>0.163164</td>\n",
       "      <td>-0.093307</td>\n",
       "      <td>1900-01-01 17:59:15.939794</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51648</th>\n",
       "      <td>51648</td>\n",
       "      <td>1.655544</td>\n",
       "      <td>8.036974</td>\n",
       "      <td>-3.199998</td>\n",
       "      <td>-0.143635</td>\n",
       "      <td>0.202538</td>\n",
       "      <td>-0.114858</td>\n",
       "      <td>1900-01-01 17:59:15.959793</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51649</th>\n",
       "      <td>51649</td>\n",
       "      <td>1.670022</td>\n",
       "      <td>8.033476</td>\n",
       "      <td>-3.199062</td>\n",
       "      <td>-0.156031</td>\n",
       "      <td>0.241975</td>\n",
       "      <td>-0.136441</td>\n",
       "      <td>1900-01-01 17:59:15.979793</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>51650 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0        ax        ay        az        gx        gy        gz  \\\n",
       "0               0  1.091892  8.707838  1.765225 -0.036396  0.063694 -0.036396   \n",
       "1               1  1.083568  8.732087  1.765318 -0.036781  0.064346 -0.036686   \n",
       "2               2  1.075247  8.756325  1.765411 -0.037166  0.064996 -0.036976   \n",
       "3               3  1.066932  8.780539  1.765503 -0.037551  0.065642 -0.037265   \n",
       "4               4  1.058625  8.804720  1.765596 -0.037936  0.066281 -0.037553   \n",
       "...           ...       ...       ...       ...       ...       ...       ...   \n",
       "51645       51645  1.611913  8.047413 -3.202833 -0.106623  0.084735 -0.050371   \n",
       "51646       51646  1.626502  8.043946 -3.201882 -0.118920  0.123886 -0.071806   \n",
       "51647       51647  1.641042  8.040466 -3.200937 -0.131261  0.163164 -0.093307   \n",
       "51648       51648  1.655544  8.036974 -3.199998 -0.143635  0.202538 -0.114858   \n",
       "51649       51649  1.670022  8.033476 -3.199062 -0.156031  0.241975 -0.136441   \n",
       "\n",
       "                        timestamp  Class Name  \n",
       "0      1900-01-01 17:44:42.000000           1  \n",
       "1      1900-01-01 17:44:42.020000           1  \n",
       "2      1900-01-01 17:44:42.040000           1  \n",
       "3      1900-01-01 17:44:42.060000           1  \n",
       "4      1900-01-01 17:44:42.080000           1  \n",
       "...                           ...         ...  \n",
       "51645  1900-01-01 17:59:15.899794           1  \n",
       "51646  1900-01-01 17:59:15.919794           1  \n",
       "51647  1900-01-01 17:59:15.939794           1  \n",
       "51648  1900-01-01 17:59:15.959793           1  \n",
       "51649  1900-01-01 17:59:15.979793           1  \n",
       "\n",
       "[51650 rows x 9 columns]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[['ax', 'ay', 'az', 'gx', 'gy', 'gz']].values   \n",
    "y = df['Class Name'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_windowed_data(X, y, window_size):\n",
    "    Xs, ys = [], []\n",
    "    for i in range(len(X) - window_size + 1):\n",
    "        Xs.append(X[i:i+window_size])   \n",
    "        ys.append(y[i+window_size-1])   \n",
    "    return np.array(Xs), np.array(ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_windowed, y_windowed = create_windowed_data(X, y, window_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(51626, 25, 6)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_windowed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_windowed, y_windowed, test_size=0.2, random_state=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.float32).unsqueeze(1)\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test = torch.tensor(y_test, dtype=torch.float32).unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([41300, 1])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "X_train, y_train, X_test, y_test = X_train.to(device), y_train.to(device), X_test.to(device), y_test.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size, dropout_prob):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout_prob)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        h_0 = torch.zeros(num_layers, x.size(0), hidden_size).to(device)   \n",
    "        c_0 = torch.zeros(num_layers, x.size(0), hidden_size).to(device)   \n",
    "        \n",
    "        out, _ = self.lstm(x, (h_0, c_0))  # LSTM forward\n",
    "        out = self.fc(out[:, -1, :])  # Son zaman adÄ±mÄ±nÄ±n Ã§Ä±ktÄ±sÄ±nÄ± alÄ±r\n",
    "        out = self.sigmoid(out)  # Sigmoid ile iki sÄ±nÄ±fa ayÄ±rÄ±r\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = X_train.shape[2]  # ax, ay, az, gx, gy, gz (6 Ã¶zellik)\n",
    "hidden_size = 24 # Gizli katman boyutu\n",
    "num_layers = 2  # LSTM katman sayÄ±sÄ±\n",
    "output_size = 1  # Binary classification\n",
    "dropout_prob = 0.5  # Dropout oranÄ±\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTMModel(input_size, hidden_size, num_layers, output_size, dropout_prob).to(device)\n",
    "criterion = nn.BCELoss()   \n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/1400], Loss: 0.6940\n",
      "Test Accuracy: 0.6133\n",
      "Epoch [20/1400], Loss: 0.6727\n",
      "Test Accuracy: 0.6905\n",
      "Epoch [30/1400], Loss: 0.6428\n",
      "Test Accuracy: 0.6908\n",
      "Epoch [40/1400], Loss: 0.5982\n",
      "Test Accuracy: 0.6917\n",
      "Epoch [50/1400], Loss: 0.5872\n",
      "Test Accuracy: 0.6982\n",
      "Epoch [60/1400], Loss: 0.5754\n",
      "Test Accuracy: 0.7147\n",
      "Epoch [70/1400], Loss: 0.5677\n",
      "Test Accuracy: 0.7207\n",
      "Epoch [80/1400], Loss: 0.5585\n",
      "Test Accuracy: 0.7242\n",
      "Epoch [90/1400], Loss: 0.5471\n",
      "Test Accuracy: 0.7272\n",
      "Epoch [100/1400], Loss: 0.5367\n",
      "Test Accuracy: 0.7295\n",
      "Epoch [110/1400], Loss: 0.5279\n",
      "Test Accuracy: 0.7349\n",
      "Epoch [120/1400], Loss: 0.5205\n",
      "Test Accuracy: 0.7436\n",
      "Epoch [130/1400], Loss: 0.5134\n",
      "Test Accuracy: 0.7509\n",
      "Epoch [140/1400], Loss: 0.5051\n",
      "Test Accuracy: 0.7592\n",
      "Epoch [150/1400], Loss: 0.4956\n",
      "Test Accuracy: 0.7631\n",
      "Epoch [160/1400], Loss: 0.4859\n",
      "Test Accuracy: 0.7682\n",
      "Epoch [170/1400], Loss: 0.4747\n",
      "Test Accuracy: 0.7737\n",
      "Epoch [180/1400], Loss: 0.4614\n",
      "Test Accuracy: 0.7868\n",
      "Epoch [190/1400], Loss: 0.4482\n",
      "Test Accuracy: 0.7976\n",
      "Epoch [200/1400], Loss: 0.4327\n",
      "Test Accuracy: 0.8073\n",
      "Epoch [210/1400], Loss: 0.4183\n",
      "Test Accuracy: 0.8174\n",
      "Epoch [220/1400], Loss: 0.4038\n",
      "Test Accuracy: 0.8259\n",
      "Epoch [230/1400], Loss: 0.3905\n",
      "Test Accuracy: 0.8351\n",
      "Epoch [240/1400], Loss: 0.3756\n",
      "Test Accuracy: 0.8441\n",
      "Epoch [250/1400], Loss: 0.3645\n",
      "Test Accuracy: 0.8506\n",
      "Epoch [260/1400], Loss: 0.3514\n",
      "Test Accuracy: 0.8585\n",
      "Epoch [270/1400], Loss: 0.3434\n",
      "Test Accuracy: 0.8655\n",
      "Epoch [280/1400], Loss: 0.3332\n",
      "Test Accuracy: 0.8702\n",
      "Epoch [290/1400], Loss: 0.3236\n",
      "Test Accuracy: 0.8755\n",
      "Epoch [300/1400], Loss: 0.3156\n",
      "Test Accuracy: 0.8781\n",
      "Epoch [310/1400], Loss: 0.3080\n",
      "Test Accuracy: 0.8824\n",
      "Epoch [320/1400], Loss: 0.2998\n",
      "Test Accuracy: 0.8865\n",
      "Epoch [330/1400], Loss: 0.2933\n",
      "Test Accuracy: 0.8888\n",
      "Epoch [340/1400], Loss: 0.2865\n",
      "Test Accuracy: 0.8904\n",
      "Epoch [350/1400], Loss: 0.2819\n",
      "Test Accuracy: 0.8926\n",
      "Epoch [360/1400], Loss: 0.2747\n",
      "Test Accuracy: 0.8950\n",
      "Epoch [370/1400], Loss: 0.2699\n",
      "Test Accuracy: 0.8972\n",
      "Epoch [380/1400], Loss: 0.2660\n",
      "Test Accuracy: 0.8999\n",
      "Epoch [390/1400], Loss: 0.2614\n",
      "Test Accuracy: 0.9011\n",
      "Epoch [400/1400], Loss: 0.2549\n",
      "Test Accuracy: 0.9030\n",
      "Epoch [410/1400], Loss: 0.2515\n",
      "Test Accuracy: 0.9036\n",
      "Epoch [420/1400], Loss: 0.2472\n",
      "Test Accuracy: 0.9065\n",
      "Epoch [430/1400], Loss: 0.2428\n",
      "Test Accuracy: 0.9092\n",
      "Epoch [440/1400], Loss: 0.2368\n",
      "Test Accuracy: 0.9109\n",
      "Epoch [450/1400], Loss: 0.2326\n",
      "Test Accuracy: 0.9121\n",
      "Epoch [460/1400], Loss: 0.2276\n",
      "Test Accuracy: 0.9150\n",
      "Epoch [470/1400], Loss: 0.2237\n",
      "Test Accuracy: 0.9159\n",
      "Epoch [480/1400], Loss: 0.2191\n",
      "Test Accuracy: 0.9178\n",
      "Epoch [490/1400], Loss: 0.2177\n",
      "Test Accuracy: 0.9203\n",
      "Epoch [500/1400], Loss: 0.2160\n",
      "Test Accuracy: 0.9210\n",
      "Epoch [510/1400], Loss: 0.2113\n",
      "Test Accuracy: 0.9216\n",
      "Epoch [520/1400], Loss: 0.2069\n",
      "Test Accuracy: 0.9229\n",
      "Epoch [530/1400], Loss: 0.2038\n",
      "Test Accuracy: 0.9249\n",
      "Epoch [540/1400], Loss: 0.1990\n",
      "Test Accuracy: 0.9268\n",
      "Epoch [550/1400], Loss: 0.1982\n",
      "Test Accuracy: 0.9282\n",
      "Epoch [560/1400], Loss: 0.1941\n",
      "Test Accuracy: 0.9300\n",
      "Epoch [570/1400], Loss: 0.1913\n",
      "Test Accuracy: 0.9306\n",
      "Epoch [580/1400], Loss: 0.1885\n",
      "Test Accuracy: 0.9324\n",
      "Epoch [590/1400], Loss: 0.1846\n",
      "Test Accuracy: 0.9342\n",
      "Epoch [600/1400], Loss: 0.1845\n",
      "Test Accuracy: 0.9364\n",
      "Epoch [610/1400], Loss: 0.1807\n",
      "Test Accuracy: 0.9360\n",
      "Epoch [620/1400], Loss: 0.1759\n",
      "Test Accuracy: 0.9361\n",
      "Epoch [630/1400], Loss: 0.1751\n",
      "Test Accuracy: 0.9360\n",
      "Epoch [640/1400], Loss: 0.1732\n",
      "Test Accuracy: 0.9378\n",
      "Epoch [650/1400], Loss: 0.1707\n",
      "Test Accuracy: 0.9396\n",
      "Epoch [660/1400], Loss: 0.1683\n",
      "Test Accuracy: 0.9382\n",
      "Epoch [670/1400], Loss: 0.1684\n",
      "Test Accuracy: 0.9401\n",
      "Epoch [680/1400], Loss: 0.1630\n",
      "Test Accuracy: 0.9433\n",
      "Epoch [690/1400], Loss: 0.1646\n",
      "Test Accuracy: 0.9424\n",
      "Epoch [700/1400], Loss: 0.1597\n",
      "Test Accuracy: 0.9431\n",
      "Epoch [710/1400], Loss: 0.1579\n",
      "Test Accuracy: 0.9418\n",
      "Epoch [720/1400], Loss: 0.1562\n",
      "Test Accuracy: 0.9447\n",
      "Epoch [730/1400], Loss: 0.1543\n",
      "Test Accuracy: 0.9446\n",
      "Epoch [740/1400], Loss: 0.1524\n",
      "Test Accuracy: 0.9462\n",
      "Epoch [750/1400], Loss: 0.1509\n",
      "Test Accuracy: 0.9486\n",
      "Epoch [760/1400], Loss: 0.1480\n",
      "Test Accuracy: 0.9477\n",
      "Epoch [770/1400], Loss: 0.1480\n",
      "Test Accuracy: 0.9483\n",
      "Epoch [780/1400], Loss: 0.1451\n",
      "Test Accuracy: 0.9488\n",
      "Epoch [790/1400], Loss: 0.1451\n",
      "Test Accuracy: 0.9502\n",
      "Epoch [800/1400], Loss: 0.1416\n",
      "Test Accuracy: 0.9525\n",
      "Epoch [810/1400], Loss: 0.1408\n",
      "Test Accuracy: 0.9520\n",
      "Epoch [820/1400], Loss: 0.1408\n",
      "Test Accuracy: 0.9535\n",
      "Epoch [830/1400], Loss: 0.1367\n",
      "Test Accuracy: 0.9528\n",
      "Epoch [840/1400], Loss: 0.1360\n",
      "Test Accuracy: 0.9532\n",
      "Epoch [850/1400], Loss: 0.1358\n",
      "Test Accuracy: 0.9534\n",
      "Epoch [860/1400], Loss: 0.1343\n",
      "Test Accuracy: 0.9537\n",
      "Epoch [870/1400], Loss: 0.1322\n",
      "Test Accuracy: 0.9542\n",
      "Epoch [880/1400], Loss: 0.1310\n",
      "Test Accuracy: 0.9557\n",
      "Epoch [890/1400], Loss: 0.1305\n",
      "Test Accuracy: 0.9564\n",
      "Epoch [900/1400], Loss: 0.1268\n",
      "Test Accuracy: 0.9572\n",
      "Epoch [910/1400], Loss: 0.1270\n",
      "Test Accuracy: 0.9569\n",
      "Epoch [920/1400], Loss: 0.1259\n",
      "Test Accuracy: 0.9572\n",
      "Epoch [930/1400], Loss: 0.1241\n",
      "Test Accuracy: 0.9577\n",
      "Epoch [940/1400], Loss: 0.1235\n",
      "Test Accuracy: 0.9584\n",
      "Epoch [950/1400], Loss: 0.1209\n",
      "Test Accuracy: 0.9590\n",
      "Epoch [960/1400], Loss: 0.1223\n",
      "Test Accuracy: 0.9596\n",
      "Epoch [970/1400], Loss: 0.1198\n",
      "Test Accuracy: 0.9576\n",
      "Epoch [980/1400], Loss: 0.1179\n",
      "Test Accuracy: 0.9610\n",
      "Epoch [990/1400], Loss: 0.1169\n",
      "Test Accuracy: 0.9603\n",
      "Epoch [1000/1400], Loss: 0.1150\n",
      "Test Accuracy: 0.9628\n",
      "Epoch [1010/1400], Loss: 0.1138\n",
      "Test Accuracy: 0.9608\n",
      "Epoch [1020/1400], Loss: 0.1153\n",
      "Test Accuracy: 0.9610\n",
      "Epoch [1030/1400], Loss: 0.1167\n",
      "Test Accuracy: 0.9620\n",
      "Epoch [1040/1400], Loss: 0.1118\n",
      "Test Accuracy: 0.9619\n",
      "Epoch [1050/1400], Loss: 0.1107\n",
      "Test Accuracy: 0.9645\n",
      "Epoch [1060/1400], Loss: 0.1065\n",
      "Test Accuracy: 0.9638\n",
      "Epoch [1070/1400], Loss: 0.1056\n",
      "Test Accuracy: 0.9644\n",
      "Epoch [1080/1400], Loss: 0.1065\n",
      "Test Accuracy: 0.9647\n",
      "Epoch [1090/1400], Loss: 0.1048\n",
      "Test Accuracy: 0.9651\n",
      "Epoch [1100/1400], Loss: 0.1043\n",
      "Test Accuracy: 0.9651\n",
      "Epoch [1110/1400], Loss: 0.1031\n",
      "Test Accuracy: 0.9650\n",
      "Epoch [1120/1400], Loss: 0.1041\n",
      "Test Accuracy: 0.9657\n",
      "Epoch [1130/1400], Loss: 0.1021\n",
      "Test Accuracy: 0.9667\n",
      "Epoch [1140/1400], Loss: 0.1001\n",
      "Test Accuracy: 0.9657\n",
      "Epoch [1150/1400], Loss: 0.1018\n",
      "Test Accuracy: 0.9651\n",
      "Epoch [1160/1400], Loss: 0.1009\n",
      "Test Accuracy: 0.9668\n",
      "Epoch [1170/1400], Loss: 0.1009\n",
      "Test Accuracy: 0.9666\n",
      "Epoch [1180/1400], Loss: 0.0995\n",
      "Test Accuracy: 0.9663\n",
      "Epoch [1190/1400], Loss: 0.0947\n",
      "Test Accuracy: 0.9667\n",
      "Epoch [1200/1400], Loss: 0.0949\n",
      "Test Accuracy: 0.9687\n",
      "Epoch [1210/1400], Loss: 0.0954\n",
      "Test Accuracy: 0.9679\n",
      "Epoch [1220/1400], Loss: 0.0962\n",
      "Test Accuracy: 0.9670\n",
      "Epoch [1230/1400], Loss: 0.0955\n",
      "Test Accuracy: 0.9678\n",
      "Epoch [1240/1400], Loss: 0.0933\n",
      "Test Accuracy: 0.9686\n",
      "Epoch [1250/1400], Loss: 0.0947\n",
      "Test Accuracy: 0.9686\n",
      "Epoch [1260/1400], Loss: 0.0914\n",
      "Test Accuracy: 0.9689\n",
      "Epoch [1270/1400], Loss: 0.0916\n",
      "Test Accuracy: 0.9684\n",
      "Epoch [1280/1400], Loss: 0.0894\n",
      "Test Accuracy: 0.9671\n",
      "Epoch [1290/1400], Loss: 0.0891\n",
      "Test Accuracy: 0.9701\n",
      "Epoch [1300/1400], Loss: 0.0878\n",
      "Test Accuracy: 0.9694\n",
      "Epoch [1310/1400], Loss: 0.0870\n",
      "Test Accuracy: 0.9695\n",
      "Epoch [1320/1400], Loss: 0.0860\n",
      "Test Accuracy: 0.9704\n",
      "Epoch [1330/1400], Loss: 0.0871\n",
      "Test Accuracy: 0.9694\n",
      "Epoch [1340/1400], Loss: 0.0857\n",
      "Test Accuracy: 0.9701\n",
      "Epoch [1350/1400], Loss: 0.0834\n",
      "Test Accuracy: 0.9703\n",
      "Epoch [1360/1400], Loss: 0.0860\n",
      "Test Accuracy: 0.9699\n",
      "Epoch [1370/1400], Loss: 0.0843\n",
      "Test Accuracy: 0.9703\n",
      "Epoch [1380/1400], Loss: 0.0831\n",
      "Test Accuracy: 0.9708\n",
      "Epoch [1390/1400], Loss: 0.0818\n",
      "Test Accuracy: 0.9714\n",
      "Epoch [1400/1400], Loss: 0.0823\n",
      "Test Accuracy: 0.9703\n"
     ]
    }
   ],
   "source": [
    " \n",
    "num_epochs = 1400\n",
    "batch_size = 32\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    output = model(X_train)\n",
    "    loss = criterion(output, y_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    " \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            y_pred = model(X_test)\n",
    "            y_pred_class = (y_pred > 0.5).float()\n",
    "            accuracy = (y_pred_class == y_test).sum().item() / y_test.size(0)\n",
    "            print(f'Test Accuracy: {accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/1400], Loss: 0.0807\n",
      "Test Accuracy: 0.9703\n",
      "Epoch [20/1400], Loss: 0.0805\n",
      "Test Accuracy: 0.9711\n",
      "Epoch [30/1400], Loss: 0.0798\n",
      "Test Accuracy: 0.9726\n",
      "Epoch [40/1400], Loss: 0.0808\n",
      "Test Accuracy: 0.9718\n",
      "Epoch [50/1400], Loss: 0.0780\n",
      "Test Accuracy: 0.9712\n",
      "Epoch [60/1400], Loss: 0.0787\n",
      "Test Accuracy: 0.9725\n",
      "Epoch [70/1400], Loss: 0.0766\n",
      "Test Accuracy: 0.9716\n",
      "Epoch [80/1400], Loss: 0.0777\n",
      "Test Accuracy: 0.9718\n",
      "Epoch [90/1400], Loss: 0.0790\n",
      "Test Accuracy: 0.9720\n",
      "Epoch [100/1400], Loss: 0.0761\n",
      "Test Accuracy: 0.9735\n",
      "Epoch [110/1400], Loss: 0.0763\n",
      "Test Accuracy: 0.9732\n",
      "Epoch [120/1400], Loss: 0.0747\n",
      "Test Accuracy: 0.9730\n",
      "Epoch [130/1400], Loss: 0.0757\n",
      "Test Accuracy: 0.9725\n",
      "Epoch [140/1400], Loss: 0.0755\n",
      "Test Accuracy: 0.9731\n",
      "Epoch [150/1400], Loss: 0.0713\n",
      "Test Accuracy: 0.9719\n",
      "Epoch [160/1400], Loss: 0.0745\n",
      "Test Accuracy: 0.9730\n",
      "Epoch [170/1400], Loss: 0.0713\n",
      "Test Accuracy: 0.9740\n",
      "Epoch [180/1400], Loss: 0.0702\n",
      "Test Accuracy: 0.9718\n",
      "Epoch [190/1400], Loss: 0.0720\n",
      "Test Accuracy: 0.9734\n",
      "Epoch [200/1400], Loss: 0.0713\n",
      "Test Accuracy: 0.9732\n",
      "Epoch [210/1400], Loss: 0.0715\n",
      "Test Accuracy: 0.9735\n",
      "Epoch [220/1400], Loss: 0.0810\n",
      "Test Accuracy: 0.9704\n",
      "Epoch [230/1400], Loss: 0.0807\n",
      "Test Accuracy: 0.9717\n",
      "Epoch [240/1400], Loss: 0.0752\n",
      "Test Accuracy: 0.9726\n",
      "Epoch [250/1400], Loss: 0.0722\n",
      "Test Accuracy: 0.9741\n",
      "Epoch [260/1400], Loss: 0.0685\n",
      "Test Accuracy: 0.9746\n",
      "Epoch [270/1400], Loss: 0.0671\n",
      "Test Accuracy: 0.9739\n",
      "Epoch [280/1400], Loss: 0.0670\n",
      "Test Accuracy: 0.9738\n",
      "Epoch [290/1400], Loss: 0.0665\n",
      "Test Accuracy: 0.9747\n",
      "Epoch [300/1400], Loss: 0.0643\n",
      "Test Accuracy: 0.9745\n",
      "Epoch [310/1400], Loss: 0.0647\n",
      "Test Accuracy: 0.9750\n",
      "Epoch [320/1400], Loss: 0.0654\n",
      "Test Accuracy: 0.9745\n",
      "Epoch [330/1400], Loss: 0.0673\n",
      "Test Accuracy: 0.9750\n",
      "Epoch [340/1400], Loss: 0.0646\n",
      "Test Accuracy: 0.9758\n",
      "Epoch [350/1400], Loss: 0.0657\n",
      "Test Accuracy: 0.9746\n",
      "Epoch [360/1400], Loss: 0.0633\n",
      "Test Accuracy: 0.9745\n",
      "Epoch [370/1400], Loss: 0.0618\n",
      "Test Accuracy: 0.9746\n",
      "Epoch [380/1400], Loss: 0.0621\n",
      "Test Accuracy: 0.9748\n",
      "Epoch [390/1400], Loss: 0.0607\n",
      "Test Accuracy: 0.9750\n",
      "Epoch [400/1400], Loss: 0.0617\n",
      "Test Accuracy: 0.9748\n",
      "Epoch [410/1400], Loss: 0.0609\n",
      "Test Accuracy: 0.9755\n",
      "Epoch [420/1400], Loss: 0.0607\n",
      "Test Accuracy: 0.9749\n",
      "Epoch [430/1400], Loss: 0.0604\n",
      "Test Accuracy: 0.9759\n",
      "Epoch [440/1400], Loss: 0.0584\n",
      "Test Accuracy: 0.9754\n",
      "Epoch [450/1400], Loss: 0.0605\n",
      "Test Accuracy: 0.9759\n",
      "Epoch [460/1400], Loss: 0.0598\n",
      "Test Accuracy: 0.9752\n",
      "Epoch [470/1400], Loss: 0.0600\n",
      "Test Accuracy: 0.9760\n",
      "Epoch [480/1400], Loss: 0.0592\n",
      "Test Accuracy: 0.9760\n",
      "Epoch [490/1400], Loss: 0.0608\n",
      "Test Accuracy: 0.9753\n",
      "Epoch [500/1400], Loss: 0.0597\n",
      "Test Accuracy: 0.9763\n",
      "Epoch [510/1400], Loss: 0.0585\n",
      "Test Accuracy: 0.9764\n",
      "Epoch [520/1400], Loss: 0.0581\n",
      "Test Accuracy: 0.9765\n",
      "Epoch [530/1400], Loss: 0.0584\n",
      "Test Accuracy: 0.9768\n",
      "Epoch [540/1400], Loss: 0.0560\n",
      "Test Accuracy: 0.9773\n",
      "Epoch [550/1400], Loss: 0.0568\n",
      "Test Accuracy: 0.9759\n",
      "Epoch [560/1400], Loss: 0.0575\n",
      "Test Accuracy: 0.9760\n",
      "Epoch [570/1400], Loss: 0.0569\n",
      "Test Accuracy: 0.9776\n",
      "Epoch [580/1400], Loss: 0.0561\n",
      "Test Accuracy: 0.9774\n",
      "Epoch [590/1400], Loss: 0.0565\n",
      "Test Accuracy: 0.9772\n",
      "Epoch [600/1400], Loss: 0.0546\n",
      "Test Accuracy: 0.9776\n",
      "Epoch [610/1400], Loss: 0.0547\n",
      "Test Accuracy: 0.9788\n",
      "Epoch [620/1400], Loss: 0.0548\n",
      "Test Accuracy: 0.9769\n",
      "Epoch [630/1400], Loss: 0.0520\n",
      "Test Accuracy: 0.9769\n",
      "Epoch [640/1400], Loss: 0.0542\n",
      "Test Accuracy: 0.9770\n",
      "Epoch [650/1400], Loss: 0.0519\n",
      "Test Accuracy: 0.9776\n",
      "Epoch [660/1400], Loss: 0.0521\n",
      "Test Accuracy: 0.9775\n",
      "Epoch [670/1400], Loss: 0.0515\n",
      "Test Accuracy: 0.9783\n",
      "Epoch [680/1400], Loss: 0.0514\n",
      "Test Accuracy: 0.9789\n",
      "Epoch [690/1400], Loss: 0.0521\n",
      "Test Accuracy: 0.9787\n",
      "Epoch [700/1400], Loss: 0.0514\n",
      "Test Accuracy: 0.9785\n",
      "Epoch [710/1400], Loss: 0.0499\n",
      "Test Accuracy: 0.9800\n",
      "Epoch [720/1400], Loss: 0.0514\n",
      "Test Accuracy: 0.9789\n",
      "Epoch [730/1400], Loss: 0.0485\n",
      "Test Accuracy: 0.9790\n",
      "Epoch [740/1400], Loss: 0.0539\n",
      "Test Accuracy: 0.9779\n",
      "Epoch [750/1400], Loss: 0.0521\n",
      "Test Accuracy: 0.9797\n",
      "Epoch [760/1400], Loss: 0.0513\n",
      "Test Accuracy: 0.9790\n",
      "Epoch [770/1400], Loss: 0.0496\n",
      "Test Accuracy: 0.9794\n",
      "Epoch [780/1400], Loss: 0.0477\n",
      "Test Accuracy: 0.9797\n",
      "Epoch [790/1400], Loss: 0.0462\n",
      "Test Accuracy: 0.9801\n",
      "Epoch [800/1400], Loss: 0.0465\n",
      "Test Accuracy: 0.9791\n",
      "Epoch [810/1400], Loss: 0.0463\n",
      "Test Accuracy: 0.9795\n",
      "Epoch [820/1400], Loss: 0.0475\n",
      "Test Accuracy: 0.9809\n",
      "Epoch [830/1400], Loss: 0.0470\n",
      "Test Accuracy: 0.9793\n",
      "Epoch [840/1400], Loss: 0.0471\n",
      "Test Accuracy: 0.9797\n",
      "Epoch [850/1400], Loss: 0.0488\n",
      "Test Accuracy: 0.9792\n",
      "Epoch [860/1400], Loss: 0.0454\n",
      "Test Accuracy: 0.9804\n",
      "Epoch [870/1400], Loss: 0.0469\n",
      "Test Accuracy: 0.9802\n",
      "Epoch [880/1400], Loss: 0.0470\n",
      "Test Accuracy: 0.9801\n",
      "Epoch [890/1400], Loss: 0.0470\n",
      "Test Accuracy: 0.9803\n",
      "Epoch [900/1400], Loss: 0.0466\n",
      "Test Accuracy: 0.9801\n",
      "Epoch [910/1400], Loss: 0.0454\n",
      "Test Accuracy: 0.9809\n",
      "Epoch [920/1400], Loss: 0.0468\n",
      "Test Accuracy: 0.9818\n",
      "Epoch [930/1400], Loss: 0.0450\n",
      "Test Accuracy: 0.9812\n",
      "Epoch [940/1400], Loss: 0.0435\n",
      "Test Accuracy: 0.9818\n",
      "Epoch [950/1400], Loss: 0.0448\n",
      "Test Accuracy: 0.9802\n",
      "Epoch [960/1400], Loss: 0.0452\n",
      "Test Accuracy: 0.9816\n",
      "Epoch [970/1400], Loss: 0.0435\n",
      "Test Accuracy: 0.9822\n",
      "Epoch [980/1400], Loss: 0.0444\n",
      "Test Accuracy: 0.9821\n",
      "Epoch [990/1400], Loss: 0.0434\n",
      "Test Accuracy: 0.9821\n",
      "Epoch [1000/1400], Loss: 0.0440\n",
      "Test Accuracy: 0.9815\n",
      "Epoch [1010/1400], Loss: 0.0730\n",
      "Test Accuracy: 0.9769\n",
      "Epoch [1020/1400], Loss: 0.0526\n",
      "Test Accuracy: 0.9826\n",
      "Epoch [1030/1400], Loss: 0.0451\n",
      "Test Accuracy: 0.9824\n",
      "Epoch [1040/1400], Loss: 0.0436\n",
      "Test Accuracy: 0.9827\n",
      "Epoch [1050/1400], Loss: 0.0429\n",
      "Test Accuracy: 0.9827\n",
      "Epoch [1060/1400], Loss: 0.0408\n",
      "Test Accuracy: 0.9828\n",
      "Epoch [1070/1400], Loss: 0.0416\n",
      "Test Accuracy: 0.9826\n",
      "Epoch [1080/1400], Loss: 0.0410\n",
      "Test Accuracy: 0.9825\n",
      "Epoch [1090/1400], Loss: 0.0403\n",
      "Test Accuracy: 0.9830\n",
      "Epoch [1100/1400], Loss: 0.0390\n",
      "Test Accuracy: 0.9830\n",
      "Epoch [1110/1400], Loss: 0.0403\n",
      "Test Accuracy: 0.9837\n",
      "Epoch [1120/1400], Loss: 0.0417\n",
      "Test Accuracy: 0.9830\n",
      "Epoch [1130/1400], Loss: 0.0396\n",
      "Test Accuracy: 0.9831\n",
      "Epoch [1140/1400], Loss: 0.0393\n",
      "Test Accuracy: 0.9832\n",
      "Epoch [1150/1400], Loss: 0.0405\n",
      "Test Accuracy: 0.9831\n",
      "Epoch [1160/1400], Loss: 0.0407\n",
      "Test Accuracy: 0.9833\n",
      "Epoch [1170/1400], Loss: 0.0406\n",
      "Test Accuracy: 0.9843\n",
      "Epoch [1180/1400], Loss: 0.0397\n",
      "Test Accuracy: 0.9844\n",
      "Epoch [1190/1400], Loss: 0.0405\n",
      "Test Accuracy: 0.9835\n",
      "Epoch [1200/1400], Loss: 0.0397\n",
      "Test Accuracy: 0.9839\n",
      "Epoch [1210/1400], Loss: 0.0393\n",
      "Test Accuracy: 0.9839\n",
      "Epoch [1220/1400], Loss: 0.0382\n",
      "Test Accuracy: 0.9845\n",
      "Epoch [1230/1400], Loss: 0.0392\n",
      "Test Accuracy: 0.9841\n",
      "Epoch [1240/1400], Loss: 0.0385\n",
      "Test Accuracy: 0.9849\n",
      "Epoch [1250/1400], Loss: 0.0414\n",
      "Test Accuracy: 0.9842\n",
      "Epoch [1260/1400], Loss: 0.0372\n",
      "Test Accuracy: 0.9838\n",
      "Epoch [1270/1400], Loss: 0.0397\n",
      "Test Accuracy: 0.9853\n",
      "Epoch [1280/1400], Loss: 0.0366\n",
      "Test Accuracy: 0.9855\n",
      "Epoch [1290/1400], Loss: 0.0390\n",
      "Test Accuracy: 0.9839\n",
      "Epoch [1300/1400], Loss: 0.0365\n",
      "Test Accuracy: 0.9845\n",
      "Epoch [1310/1400], Loss: 0.0376\n",
      "Test Accuracy: 0.9852\n",
      "Epoch [1320/1400], Loss: 0.0381\n",
      "Test Accuracy: 0.9844\n",
      "Epoch [1330/1400], Loss: 0.0412\n",
      "Test Accuracy: 0.9843\n",
      "Epoch [1340/1400], Loss: 0.0585\n",
      "Test Accuracy: 0.9839\n",
      "Epoch [1350/1400], Loss: 0.0443\n",
      "Test Accuracy: 0.9816\n",
      "Epoch [1360/1400], Loss: 0.0425\n",
      "Test Accuracy: 0.9839\n",
      "Epoch [1370/1400], Loss: 0.0396\n",
      "Test Accuracy: 0.9854\n",
      "Epoch [1380/1400], Loss: 0.0385\n",
      "Test Accuracy: 0.9859\n",
      "Epoch [1390/1400], Loss: 0.0343\n",
      "Test Accuracy: 0.9856\n",
      "Epoch [1400/1400], Loss: 0.0354\n",
      "Test Accuracy: 0.9853\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 1400\n",
    "batch_size = 32\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    output = model(X_train)\n",
    "    loss = criterion(output, y_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    " \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            y_pred = model(X_test)\n",
    "            y_pred_class = (y_pred > 0.5).float()\n",
    "            accuracy = (y_pred_class == y_test).sum().item() / y_test.size(0)\n",
    "            print(f'Test Accuracy: {accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2400 epoch batcsize =32  window_size=25 (sampling rate yarÄ±sÄ±), accurac 0.9853 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to lstm_falling_detection_modelv5_9853.pth\n"
     ]
    }
   ],
   "source": [
    "MODEL_PATH = \"lstm_falling_detection_modelv5_9853.pth\"\n",
    "torch.save(model.state_dict(), MODEL_PATH)\n",
    "print(f\"Model saved to {MODEL_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model ve optimizer durumlarÄ± kaydedildi.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "torch.save({\n",
    "    'epoch': num_epochs,\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'loss': loss.item(),\n",
    "}, 'lstm_model_checkpoint_model_v5_9853.pth')\n",
    "print(\"Model ve optimizer durumlarÄ± kaydedildi.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchsummary  import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTMModel(\n",
       "  (lstm): LSTM(6, 24, num_layers=2, batch_first=True, dropout=0.5)\n",
       "  (fc): Linear(in_features=24, out_features=1, bias=True)\n",
       "  (sigmoid): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
