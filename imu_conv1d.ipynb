{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler,MinMaxScaler,LabelEncoder\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, Subset, random_split\n",
    "from torchvision import transforms\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as f\n",
    "from PIL import Image\n",
    "import os\n",
    "from sklearn import metrics\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"data_imu_birlestirilmis.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder=LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Class Name\"]=label_encoder.fit_transform(df[\"Class Name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe=df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 sayısı: 15978\n",
      "1 sayısı: 35672\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class_counts = df['Class Name'].value_counts()\n",
    "\n",
    "print(\"0 sayısı:\", class_counts.get(0, 0))  \n",
    "print(\"1 sayısı:\", class_counts.get(1, 0))   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 51650 entries, 0 to 51649\n",
      "Data columns (total 9 columns):\n",
      " #   Column      Non-Null Count  Dtype  \n",
      "---  ------      --------------  -----  \n",
      " 0   Unnamed: 0  51650 non-null  int64  \n",
      " 1   ax          51650 non-null  float64\n",
      " 2   ay          51650 non-null  float64\n",
      " 3   az          51650 non-null  float64\n",
      " 4   gx          51650 non-null  float64\n",
      " 5   gy          51650 non-null  float64\n",
      " 6   gz          51650 non-null  float64\n",
      " 7   timestamp   51650 non-null  object \n",
      " 8   Class Name  51650 non-null  int32  \n",
      "dtypes: float64(6), int32(1), int64(1), object(1)\n",
      "memory usage: 3.3+ MB\n"
     ]
    }
   ],
   "source": [
    "dataframe.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# son zaman adım etiketi alındı , bu değiştirilebilir , istatistik olarak daha fazla olanı tercih ediyorum ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "class CNNDataset(Dataset):\n",
    "    \n",
    "    def __init__ (self, dataframe, sequence_length=25):\n",
    "        self.dataframe = dataframe\n",
    "        self.features = [\"ax\", \"ay\", \"az\", \"gx\", \"gy\", \"gz\"]\n",
    "        self.labels = \"Class Name\"\n",
    "        self.sequence_length = sequence_length\n",
    "        self.scaler = StandardScaler()\n",
    "        self.dataframe[self.features] = self.scaler.fit_transform(self.dataframe[self.features])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataframe) - self.sequence_length + 1\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        features_sequence = self.dataframe.loc[idx:idx + self.sequence_length - 1, self.features].values.astype(float)\n",
    "        #son zaman adımı etiketi alındı . bu değiştirilebilir\n",
    "        label = self.dataframe.loc[idx + self.sequence_length - 1, self.labels]\n",
    "        features_tensor = torch.tensor(features_sequence, dtype=torch.float32).permute(1, 0)   \n",
    "        label_tensor = torch.tensor(label, dtype=torch.long)\n",
    "        \n",
    "        return features_tensor, label_tensor\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = CNNDataset(dataframe)\n",
    "\n",
    " \n",
    "labels = dataset.dataframe['Class Name'].values[:len(dataset)]   \n",
    "\n",
    " \n",
    "indices = list(range(len(dataset)))\n",
    "\n",
    " \n",
    "train_indices, test_indices = train_test_split(indices, test_size=0.2, stratify=labels)\n",
    " \n",
    "train_dataset = torch.utils.data.Subset(dataset, train_indices)\n",
    "test_dataset = torch.utils.data.Subset(dataset, test_indices)\n",
    "\n",
    " \n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 6, 25])\n",
      "torch.Size([32])\n",
      "torch.Size([32, 6, 25])\n",
      "torch.Size([32])\n",
      "torch.Size([32, 6, 25])\n",
      "torch.Size([32])\n",
      "torch.Size([32, 6, 25])\n",
      "torch.Size([32])\n",
      "torch.Size([32, 6, 25])\n",
      "torch.Size([32])\n",
      "torch.Size([32, 6, 25])\n",
      "torch.Size([32])\n",
      "torch.Size([32, 6, 25])\n",
      "torch.Size([32])\n",
      "torch.Size([32, 6, 25])\n",
      "torch.Size([32])\n",
      "torch.Size([32, 6, 25])\n",
      "torch.Size([32])\n",
      "torch.Size([32, 6, 25])\n",
      "torch.Size([32])\n",
      "torch.Size([32, 6, 25])\n",
      "torch.Size([32])\n",
      "torch.Size([32, 6, 25])\n",
      "torch.Size([32])\n",
      "torch.Size([32, 6, 25])\n",
      "torch.Size([32])\n",
      "torch.Size([32, 6, 25])\n",
      "torch.Size([32])\n",
      "torch.Size([32, 6, 25])\n",
      "torch.Size([32])\n",
      "torch.Size([32, 6, 25])\n",
      "torch.Size([32])\n",
      "torch.Size([32, 6, 25])\n",
      "torch.Size([32])\n",
      "torch.Size([32, 6, 25])\n",
      "torch.Size([32])\n",
      "torch.Size([32, 6, 25])\n",
      "torch.Size([32])\n",
      "torch.Size([32, 6, 25])\n",
      "torch.Size([32])\n",
      "torch.Size([32, 6, 25])\n",
      "torch.Size([32])\n",
      "torch.Size([32, 6, 25])\n",
      "torch.Size([32])\n",
      "torch.Size([32, 6, 25])\n",
      "torch.Size([32])\n",
      "torch.Size([32, 6, 25])\n",
      "torch.Size([32])\n",
      "torch.Size([32, 6, 25])\n",
      "torch.Size([32])\n",
      "torch.Size([32, 6, 25])\n",
      "torch.Size([32])\n",
      "torch.Size([32, 6, 25])\n",
      "torch.Size([32])\n",
      "torch.Size([32, 6, 25])\n",
      "torch.Size([32])\n",
      "torch.Size([32, 6, 25])\n",
      "torch.Size([32])\n",
      "torch.Size([32, 6, 25])\n",
      "torch.Size([32])\n",
      "torch.Size([32, 6, 25])\n",
      "torch.Size([32])\n",
      "torch.Size([32, 6, 25])\n",
      "torch.Size([32])\n",
      "torch.Size([32, 6, 25])\n",
      "torch.Size([32])\n",
      "torch.Size([32, 6, 25])\n",
      "torch.Size([32])\n",
      "torch.Size([32, 6, 25])\n",
      "torch.Size([32])\n",
      "torch.Size([32, 6, 25])\n",
      "torch.Size([32])\n",
      "torch.Size([32, 6, 25])\n",
      "torch.Size([32])\n",
      "torch.Size([32, 6, 25])\n",
      "torch.Size([32])\n",
      "torch.Size([32, 6, 25])\n",
      "torch.Size([32])\n",
      "torch.Size([32, 6, 25])\n",
      "torch.Size([32])\n",
      "torch.Size([32, 6, 25])\n",
      "torch.Size([32])\n",
      "torch.Size([32, 6, 25])\n",
      "torch.Size([32])\n",
      "torch.Size([32, 6, 25])\n",
      "torch.Size([32])\n",
      "torch.Size([32, 6, 25])\n",
      "torch.Size([32])\n",
      "torch.Size([32, 6, 25])\n",
      "torch.Size([32])\n",
      "torch.Size([32, 6, 25])\n",
      "torch.Size([32])\n",
      "torch.Size([32, 6, 25])\n",
      "torch.Size([32])\n",
      "torch.Size([32, 6, 25])\n",
      "torch.Size([32])\n",
      "torch.Size([32, 6, 25])\n",
      "torch.Size([32])\n",
      "torch.Size([32, 6, 25])\n",
      "torch.Size([32])\n",
      "torch.Size([32, 6, 25])\n",
      "torch.Size([32])\n",
      "torch.Size([32, 6, 25])\n",
      "torch.Size([32])\n",
      "torch.Size([32, 6, 25])\n",
      "torch.Size([32])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m x,y  \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;28mprint\u001b[39m(x\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(y\u001b[38;5;241m.\u001b[39mshape)\n",
      "File \u001b[1;32mc:\\Users\\ninja\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\ninja\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:673\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    671\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    672\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 673\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    674\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    675\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\ninja\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:50\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[0;32m     49\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__getitems__\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__:\n\u001b[1;32m---> 50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__getitems__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\ninja\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\utils\\data\\dataset.py:420\u001b[0m, in \u001b[0;36mSubset.__getitems__\u001b[1;34m(self, indices)\u001b[0m\n\u001b[0;32m    418\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m    419\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 420\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx]] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "File \u001b[1;32mc:\\Users\\ninja\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\utils\\data\\dataset.py:420\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    418\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m    419\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 420\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "Cell \u001b[1;32mIn[15], line 29\u001b[0m, in \u001b[0;36mCNNDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     26\u001b[0m label \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataframe\u001b[38;5;241m.\u001b[39mloc[idx \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msequence_length \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabels]\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# Özellikleri tensöre dönüştürüp, uygun formata (batch, channels, sequence) sokuyoruz\u001b[39;00m\n\u001b[1;32m---> 29\u001b[0m features_tensor \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures_sequence\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m)  \u001b[38;5;66;03m# (features, sequence_length)\u001b[39;00m\n\u001b[0;32m     30\u001b[0m label_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(label, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong)\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m# Conv1D için (channels, sequence_length) formatında olmalı.\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for x,y  in train_loader:\n",
    "    print(x.shape)\n",
    "    print(y.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "class ImprovedCNNModel1D(nn.Module):\n",
    "    def __init__(self, input_length):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv1d(6, 32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv1d(32, 64, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv1d(64, 128, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool1d(2)\n",
    "        \n",
    "        # Calculate the size of the feature map after convolution and pooling layers\n",
    "        def conv1d_output_size(length, kernel_size=3, padding=1, stride=1):\n",
    "            return (length + 2*padding - (kernel_size - 1) - 1) // stride + 1\n",
    "\n",
    "        def pool1d_output_size(length, kernel_size=2, stride=2):\n",
    "            return (length - (kernel_size - 1) - 1) // stride + 1\n",
    "        \n",
    "        l = input_length\n",
    "        l = conv1d_output_size(l)  # conv1\n",
    "        l = pool1d_output_size(l)  # pool1\n",
    "        l = conv1d_output_size(l)  # conv2\n",
    "        l = pool1d_output_size(l)  # pool2\n",
    "        l = conv1d_output_size(l)  # conv3\n",
    "        l = pool1d_output_size(l)  # pool3\n",
    "        \n",
    "        self.flattened_size = 128 * l  # 128 channels after conv3 and final pooled length\n",
    "\n",
    "        self.fc1 = nn.Linear(self.flattened_size, 512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc3 = nn.Linear(256, 1)  # Output a single value for binary classification\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(nn.functional.relu(self.conv1(x)))\n",
    "        x = self.pool(nn.functional.relu(self.conv2(x)))\n",
    "        x = self.pool(nn.functional.relu(self.conv3(x)))\n",
    "        x = x.view(-1, self.flattened_size)\n",
    "        x = nn.functional.relu(self.fc1(x))\n",
    "        x = nn.functional.relu(self.fc2(x))\n",
    "        x = torch.sigmoid(self.fc3(x))  # Sigmoid to output a probability between 0 and 1\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImprovedCNNModel1D(nn.Module):\n",
    "    def __init__(self, input_length, dropout_rate=0.5):  \n",
    "        super().__init__()\n",
    "        \n",
    "        # Conv1d katmanları\n",
    "        self.conv1 = nn.Conv1d(6, 32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv1d(32, 64, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv1d(64, 128, kernel_size=3, padding=1)\n",
    "        \n",
    "        # MaxPool katmanı\n",
    "        self.pool = nn.MaxPool1d(2)\n",
    "        \n",
    "        # Dropout \n",
    "        self.dropout = nn.Dropout(dropout_rate)  # Dropout katmanı  \n",
    "        \n",
    "        # Calculate the size of the feature map after convolution and pooling layers\n",
    "        def conv1d_output_size(length, kernel_size=3, padding=1, stride=1):\n",
    "            return (length + 2*padding - (kernel_size - 1) - 1) // stride + 1\n",
    "\n",
    "        def pool1d_output_size(length, kernel_size=2, stride=2):\n",
    "            return (length - (kernel_size - 1) - 1) // stride + 1\n",
    "        \n",
    "        l = input_length\n",
    "        l = conv1d_output_size(l)  # conv1\n",
    "        l = pool1d_output_size(l)  # pool1\n",
    "        l = conv1d_output_size(l)  # conv2\n",
    "        l = pool1d_output_size(l)  # pool2\n",
    "        l = conv1d_output_size(l)  # conv3\n",
    "        l = pool1d_output_size(l)  # pool3\n",
    "        \n",
    "        self.flattened_size = 128 * l   \n",
    "\n",
    "        self.fc1 = nn.Linear(self.flattened_size, 512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc3 = nn.Linear(256, 1)   \n",
    "\n",
    "    def forward(self, x):\n",
    "         \n",
    "        x = self.pool(nn.functional.relu(self.conv1(x)))\n",
    "        x = self.dropout(x)   \n",
    "        x = self.pool(nn.functional.relu(self.conv2(x)))\n",
    "        x = self.dropout(x)   \n",
    "        x = self.pool(nn.functional.relu(self.conv3(x)))\n",
    "        x = self.dropout(x)   \n",
    "        \n",
    "         \n",
    "        x = x.view(-1, self.flattened_size)\n",
    "        \n",
    "         \n",
    "        x = nn.functional.relu(self.fc1(x))\n",
    "        x = self.dropout(x)   \n",
    "        x = nn.functional.relu(self.fc2(x))\n",
    "        x = self.dropout(x)  \n",
    "        x = torch.sigmoid(self.fc3(x))  # Sigmoid ile 0-1 arası olasılık\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCELoss()  # Binary Cross-Entropy loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "model = ImprovedCNNModel1D(input_length=25)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device=torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 0.4188, Accuracy: 80.62%\n",
      "Epoch [2/10], Loss: 0.2559, Accuracy: 89.17%\n",
      "Epoch [3/10], Loss: 0.1655, Accuracy: 93.30%\n",
      "Epoch [4/10], Loss: 0.1151, Accuracy: 95.50%\n",
      "Epoch [5/10], Loss: 0.0885, Accuracy: 96.59%\n",
      "Epoch [6/10], Loss: 0.0772, Accuracy: 97.20%\n",
      "Epoch [7/10], Loss: 0.0645, Accuracy: 97.69%\n",
      "Epoch [8/10], Loss: 0.0591, Accuracy: 97.85%\n",
      "Epoch [9/10], Loss: 0.0556, Accuracy: 97.97%\n",
      "Epoch [10/10], Loss: 0.0497, Accuracy: 98.27%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "model = ImprovedCNNModel1D(input_length=25).to(device)   \n",
    "criterion = nn.BCELoss()   \n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  \n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for batch_idx, (data, labels) in enumerate(train_loader):\n",
    "        data, labels = data.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        #forward pass \n",
    "        outputs = model(data)\n",
    "        outputs = outputs.squeeze(1)  \n",
    "\n",
    "        # Calculate loss\n",
    "        loss = criterion(outputs, labels.float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        predicted = (outputs >= 0.5).float()\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "    avg_loss = running_loss / len(train_loader)\n",
    "    accuracy = 100 * correct / total\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}, Accuracy: {accuracy:.2f}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 0.0468, Accuracy: 98.36%\n",
      "Epoch [2/10], Loss: 0.0470, Accuracy: 98.33%\n",
      "Epoch [3/10], Loss: 0.0430, Accuracy: 98.48%\n",
      "Epoch [4/10], Loss: 0.0406, Accuracy: 98.56%\n",
      "Epoch [5/10], Loss: 0.0384, Accuracy: 98.66%\n",
      "Epoch [6/10], Loss: 0.0400, Accuracy: 98.65%\n",
      "Epoch [7/10], Loss: 0.0352, Accuracy: 98.78%\n",
      "Epoch [8/10], Loss: 0.0373, Accuracy: 98.79%\n",
      "Epoch [9/10], Loss: 0.0357, Accuracy: 98.74%\n",
      "Epoch [10/10], Loss: 0.0347, Accuracy: 98.81%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  \n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for batch_idx, (data, labels) in enumerate(train_loader):\n",
    "        data, labels = data.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad() \n",
    "        outputs = model(data)\n",
    "        outputs = outputs.squeeze(1)  \n",
    "        loss = criterion(outputs, labels.float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        predicted = (outputs >= 0.5).float()\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "    avg_loss = running_loss / len(train_loader)\n",
    "    accuracy = 100 * correct / total\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}, Accuracy: {accuracy:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 98.67%\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():  \n",
    "    for batch_idx, (data, labels) in enumerate(test_loader):\n",
    "        data, labels = data.to(device), labels.to(device)\n",
    "\n",
    "        outputs = model(data)\n",
    "        outputs = outputs.squeeze(1)  # Bce loss için\n",
    "\n",
    "        predicted = (outputs >= 0.5).float()\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "accuracy = 100 * correct / total\n",
    "print(f\"Test Accuracy: {accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
